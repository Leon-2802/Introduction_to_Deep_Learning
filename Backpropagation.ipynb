{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPVj9D5Yisb1sa6oJEPcnyr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"id":"ICuPQ2vkeWWB","executionInfo":{"status":"error","timestamp":1670587328164,"user_tz":-60,"elapsed":221,"user":{"displayName":"Alexander Weibert","userId":"08548358091748129792"}},"outputId":"1fe6e7f7-99a5-427c-b493-c1a0e312b571"},"outputs":[{"output_type":"stream","name":"stdout","text":["sigmoid\n"," [[0.58904043 0.61538376]\n"," [0.70266065 0.7251195 ]]\n","forwardProp\n"," [[0.36 0.47]\n"," [0.86 0.97]] \n","\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-2ba776b0b733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforwardProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mforwardProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-59-2ba776b0b733>\u001b[0m in \u001b[0;36mforwardProp\u001b[0;34m(input, weights, bias, activation)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforwardProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m  \u001b[0;31m# transpose, as tensorflow transposes internally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: shapes (2,2) and (3,2) not aligned: 2 (dim 1) != 3 (dim 0)"]}],"source":["import numpy as np\n","\n","input = np.array(([0.1],\n","                   [0.8]))\n","\n","weights1 = np.array(([0.2, 0.5],\n","                      [0.3, 0.4]))\n","\n","bias1 = np.array(([0.1],\n","                   [0.6]))\n","\n","weights2 = np.array(([0.1, 0.3],\n","                      [0.7, 0.4],\n","                      [0.01, 0.02]))\n","\n","bias2 = np.array(([0.2],\n","                   [0.1],\n","                   [0.4]))\n","\n","def forwardProp(input, weights, bias, activation):\n","  Z = np.dot(input.T, weights) + bias  # transpose, as tensorflow transposes internally\n","  if activation == \"sigmoid\":\n","    A = sigmoid(Z)\n","  elif activation == \"softmax\":\n","    A = softmax(Z)\n","  print(\"forwardProp\\n\", Z, \"\\n\")\n","  return A, Z\n","\n","def softmax(input):\n","    t = np.exp(input)\n","    t = t / t.sum(axis=0, keepdims=True)\n","    print(\"softmax\\n\", t, \"\\n\")\n","    return t\n","\n","def sigmoid(input):\n","  A = 1 / (1 + np.exp(-input))\n","  print(\"sigmoid\\n\", A)\n","  return A\n","\n","a, z = forwardProp(input, weights1, bias1, \"sigmoid\")\n","\n","forwardProp(z, weights2, bias2, \"softmax\")\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.InputLayer(input_shape=(2,)))\n","model.add(tf.keras.layers.Dense(2, activation='sigmoid',kernel_initializer=tf.keras.initializers.Constant(np.transpose(weights1)),\n","    bias_initializer=tf.keras.initializers.Constant(np.transpose(bias1))))\n","model.add(tf.keras.layers.Dense(3, activation='softmax',kernel_initializer=tf.keras.initializers.Constant(np.transpose(weights2)),\n","    bias_initializer=tf.keras.initializers.Constant(np.transpose(bias2))))\n","\n","\n","\n","with tf.GradientTape() as tape:\n","  # Forward pass\n","  tape.watch(tf.constant(input))\n","  f_output = model(np.transpose(input))\n","  loss = tf.keras.losses.CategoricalCrossentropy()\n","  cce = loss(np.transpose(y),f_output)\n","  tape.watch(cce)\n","# Calculate gradients with respect to every trainable variable\n","grad = tape.gradient(cce, model.trainable_variables)\n","for var, g in zip(model.trainable_variables, grad):\n","  print(f'{var.name}, shape: {g.shape}')\n","print(grad)"],"metadata":{"id":"7W0SkSdqqsx8"},"execution_count":null,"outputs":[]}]}